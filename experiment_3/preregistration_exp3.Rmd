---
title: "Pre-registration Experiment 3"
output:
  bookdown::pdf_document2:
    toc: false
  bookdown::word_document2: default
  bookdown::html_document2:
    keep_md: yes
    toc: false  # Remove table of contents from HTML output
always_allow_html: true
---


```{r, include=FALSE}
# load packages
library(tidyverse)
library(kableExtra)
library(lme4)
```


```{r, include=FALSE}
# generate some random data with respective variables to ensure 
# using code snippets that work 

# simulate data
data <- tibble(
  convergence = sample(0:3, replace = TRUE, size=100),
  accuracy = sample(0:100, replace = TRUE, size=100),
  competence = sample(1:7, replace = TRUE, size=100),
  condition = sample(c("3", "10", "10_distant"), replace = TRUE, 
                   prob = c(0.5, 0.25, 0.25), size = 100),
  id = rep(1:5, length.out = 100)
  ) %>% 
  mutate(stimuli_10_version = ifelse(str_detect(condition, "3"), NA, 
                                     condition), 
         options = ifelse(str_detect(condition, "10"), "10", 
                          condition)
                                     )
```

# I. Introduction

Does people's knowledge about science vanish faster than their impression science being trustworthy? 

The deficit model suggests that people do not trust science (enough), because they do not know enough about it. Accordingly, more knowledge about science generates trust. Here, we want to contrast this model with an alternative one: the trust-by-impression account. According to this model, people trust science not so much because they know about it, but because they have been impressed by it.

An implication of the deficit model is that trust should only co-occur with knowledge. For example, someone who doesn't have a minimum of astronomy knowledge (e.g. doesn't know that the earth revolves around the sun) has no reason to trust astronomers at all. By contrast, the trust-by-impression account could explain why someone without much knowledge might nevertheless trust astronomers: it requires only that a person has (at some point) been impressed by a finding of astronomers. They might have forgotten the details or even what it was, but the impression of trustworthiness remains. 

In two previous experiments, we have shown that exposure to impressive scientific content increases trust in scientists. In this experiment, we test if this trust in scientists persists, and if it persists more than specific knowledge about the content.


```{r}
# Alternative design

# In line with both accounts, we first need to show that exposure to impressive scientific content increases trust in science, and persistently so. In a second step, we can then test if this newly induced trust persists more than the knowledge its content. The deficit model suggests that knowledge and trust persist to the same extent. If, however, knowledge vanishes while trust persists, this would be evidence in favor of the trust-by-impression account.

# We propose the following design. We have three experimental group: (i) a baseline control group that receives a non-impressive science vignette (as established by previous experiments), (ii) a treatment group that receives an impressive science vignette, and (iii) a second treatment group that also receives an impressive science vignette, but which answers questions after a decoy task of 5 to 10mins.

# steps: 
# a) new information changes trust in science (baseline control vs. impressive treatment.)
# b) trust in science persists more than science knowledge (impressive treatment vs. impressive + decoy treatment)
```

# II. Data collection

No data has been collected yet. We ran a pilot study to investigate how people do for the knowledge quizz about the vignettes. The results suggested that people do rather well overall, but are not at ceiling either. We are therefore confident that our findings will not be overly influenced by ceiling/floor effects. 

[show graphs of pilot 5 here].

# III. Design

All participants read a vignette (established as impressive and trust enhancing in previous experiments). We randomize whether this vignette is about archaeology or entomology. We then randomly assign participants to two groups: in the control group, people proceed right after having read the vignette to answer all questions about the content of the vignettes and their trust in archeologists/entomologists; in the treatment group, participants engage in a 5-10min decoy task before proceeding to the questions. 

To make the decoy less obvious, participants in the treatment group will answer two irrelevant questions about the vignette before proceeding with the (unrelated) decoy. 

```{r -stimulus, echo=FALSE, fig.cap="An example of a stimulus for the majority condition"}
knitr::include_graphics("figures/example_stimulus.png")
```

## Materials

These are the vignettes XX

## Measures

These are the measures XX

**Convergence**. Convergence varies by the ratio of players choosing the same response as the focal player (i.e. the one that participants evaluate). The levels of convergence are: (i) consensus, where all three players pick the same option [`coded value = 3`]; (ii) majority, where either the third or second player picks the same option as the first player [`coded value = 2`]; (iii) dissensus, where all three players pick different options [`coded value = 1`]; (iv) majority against the focal player's estimate, where the second and third player pick the same option, but one that is different from the first player's choice [`coded value = 0`]. In our analysis, we treat convergence as a continuous variable, assigning the values in squared parenthesis.

We manipulate convergence within participants. All participants see all four conditions of convergence, with two stimuli (i.e. game results) per condition. Each participant therefore sees eight stimuli in total (4 convergence levels x 2 stimuli) .

```{r stimuli-3, echo=FALSE}
# Create a matrix of image file paths as Markdown-formatted strings
image_paths <- data.frame(condition = c("opposing majority (0)", 
                                        "dissensus (1)", "majority (2)", 
                                        "consensus (3)"),
                          imgage_a = c("![](figures/stimuli/minority_3_a.png){ width=60% }", 
                        "![](figures/stimuli/divergence_3_a.png){ width=60% }",
                        "![](figures/stimuli/majority_3_a.png){ width=60% }", 
                        "![](figures/stimuli/consensus_3_a.png){ width=60% }"),
                        imgage_b = c("![](figures/stimuli/minority_3_b.png){ width=60% }", 
                        "![](figures/stimuli/divergence_3_b.png){ width=60% }",
                        "![](figures/stimuli/majority_3_b.png){ width=60% }", 
                        "![](figures/stimuli/consensus_3_b.png){ width=60% }"))

# Use kable() to create the table and print it as Markdown
kableExtra::kable(image_paths, format = "markdown",
                  col.names = c("Level", "Version a)", "Version b)"), 
                  align = "c",
                  caption = "Stimuli for 3 options condition by levels of convergence")

```

**Number of choice options**. Number of options has two levels: '3' '10'. 

We manipulate number of choice options between participants, i.e. each participant gets assigned to either the '3' or the '10' condition. All stimuli that participants see in their respective condition will involve the same number of choice options. 

As for the '10' options condition, participants will see one of two distinct set of stimuli (`stimuli_10_version`): one in which the range of the answers corresponds to the range of the '3' options condition (`stimuli_10_version == 10`), and another with increased range (`stimuli_10_version == 10_distant`; see Appendix). We add the increased range condition because we think that there is the possibility that participant might not consider all options as relevant when they only see scenarios in which all answers cluster. For the main analyses, we will simply merge these two stimuli sets and treat them as part of the same '10' options condition. However, we will investigate potential differences as a research question. 

```{r stimuli-options, echo=FALSE}
# Create a matrix of image file paths as Markdown-formatted strings
image_paths <- data.frame(convergence = c("consensus (3)"),
                          imgage_a = c("![](figures/stimuli/consensus_3_a.png){ width=60% }"),
                        imgage_b = c("![](figures/stimuli/consensus_10_a.png){ width=60% }"))

# Use kable() to create the table and print it as Markdown
kableExtra::kable(image_paths, format = "markdown",
                  col.names = c("Convergence", "Number of options: 3", "Number of options: 10"), 
                  align = "c",
                  caption = "Example of a consensus stimulus for the two 'Number of option' conditions")
```

As outcome variables, we will measure people's perceived accuracy and competence of player one.

**Accuracy**. We ask participants "What do you think is the probability of player 1 being correct?". Participants answer with a slider from 0 to 100.

**Competence**. We ask participants "How competent do you think player 1 is in games like these?" Participants answer on a 7-point Likert scale (from "not competent at all" to "extremely competent").

# IV. Hypotheses

Considering only `3` options condition of the experiment at hand, we expect the findings of experiment four to replicate, for it is the exact same experiment as experiment four in this case. 

### H1a: In the three options condition, participants perceive an estimate of an independent informant as more accurate the more it converges with the estimates of other informants.

To test this hypothesis, we only consider participants assigned to the `3` options condition.

We use a linear mixed effect model with random intercept and random slope per participant. Should this model yield convergence issues, we will use a model with random intercept only.

In all our models we treat `convergence` as a continuous variable. We will, however, include robustness checks where we treat convergence as a categorical variable, allowing to inspect difference between different levels.

```{r, warning=FALSE, message=FALSE}
# models for accuracy

# random intercept and slope by participants
model_accuracy <- lmer(accuracy ~ convergence + (1 + convergence | id), 
                       data = data %>% filter(options == "3"))

# in case of non-convergence: random intercept by participants only
alt_model_accuracy <- lmer(accuracy ~ convergence + (1 | id), 
                           data = data %>% filter(options == "3"))
```

### H1b:  In the three options condition, participants perceive an independent informant as more competent the more their estimate converges with the estimates of other informants.

To test this hypothesis, we only consider participants assigned to the `3` options condition.

We will proceed in the same way for `competence` as we did for `accuracy` above.

```{r, warning=FALSE, message=FALSE}
# models for competence

# random intercept and slope by participants
model_competence <- lmer(competence ~ convergence + 
                           (1 + convergence | id), 
                         data = data %>% filter(options == "3"))

# in case of non-convergence: random intercept by participants only
alt_model_competence <- lmer(competence ~ convergence + (1 | id), 
                             data = data %>% filter(options == "3"))
```

How about a context in which informants (i.e. the fictive players) could choose not among 3, but among 10 options? Following the results from our model, we predict that effects of convergence are more positive when there are more choice options: 

### H2a: The effect of convergence on accuracy (H1a) is more positive in a context when informants can choose among ten response options compared to when they can choose among only three. 

To test this hypothesis, we consider the full data.

The resulting estimate of the interaction term will provide the test for our hypothesis. 

```{r, warning=FALSE, message=FALSE}
# models for accuracy

# random intercept and slope by participants
model_accuracy <- lmer(accuracy ~ convergence + options + 
                            options*convergence + (1 + convergence | id), 
                       data = data)

# in case of non-convergence: random intercept by participants only
alt_model_accuracy <- lmer(accuracy ~ convergence + options + 
                            options*convergence + (1 | id), 
                           data = data)
```


### H2b: The effect of convergence on competence (H1b) is more positive in a context when informants can choose among ten response options compared to when they can choose among only three. 

To test this hypothesis, we consider the full data.

The resulting estimate of the interaction term will provide the test for our hypothesis. 

```{r, warning=FALSE, message=FALSE}
# models for competence

# random intercept and slope by participants
model_competence <- lmer(competence ~ convergence + options + 
                            options*convergence + (1 + convergence | id), 
                       data = data)

# in case of non-convergence: random intercept by participants only
alt_model_competence <- lmer(competence ~ convergence + options + 
                            options*convergence + (1 | id), 
                           data = data)
```

## Research question 

### RQ1: Within the 10 choice options condition, is the effect of convergence more positive for the set of stimuli with greater distance?

```{r, warning=FALSE, message=FALSE}
# models for accuracy

# random intercept and slope by participants
model_accuracy <- lmer(accuracy ~ convergence + stimuli_10_version + 
                            stimuli_10_version*convergence + (1 + convergence | id), 
                       data = data)

# in case of non-convergence: random intercept by participants only
alt_model_accuracy <- lmer(accuracy ~ convergence + stimuli_10_version + 
                            stimuli_10_version*convergence + (1 | id), 
                           data = data)

# models for competence

# random intercept and slope by participants
model_competence <- lmer(competence ~ convergence + stimuli_10_version + 
                            stimuli_10_version*convergence + (1 + convergence | id), 
                       data = data)

# in case of non-convergence: random intercept by participants only
alt_model_competence <- lmer(competence ~ convergence + stimuli_10_version + 
                            stimuli_10_version*convergence + (1 | id), 
                           data = data)
```

Should we find the difference, we will re-run the analyses of the second set of hypotheses restraining the data to the subset of the more distant set of stimuli for the 10 options condition. 

Pick an example stimulus as earlier for illustrating choice options. See all stimuli in appendix.

# Robustness checks

## Convergence as categorical variable

In the models above, we treated convergence as a continuous variable. Based on the different levels, we will build a categorical variable, `convergence_categorical`.

```{r}
# make a categorical variable from `convergence`
data <- data %>% 
  mutate(convergence_categorical = recode_factor(convergence, 
                                                 `0` = "opposing majority", 
                                                 `1` = "divergence", 
                                                 `2` = "majority", 
                                                 `3` = "consensus",
                                                 .default = NA_character_)
         )

levels(data$convergence_categorical)
  
```

We run the same models outlined in the hypotheses section, but replacing `convergence` with `convergence_categorical`. This also allows us to inspect heterogeneity in differences between levels (with respect to the baseline, i.e. "opposing majority").

# Exclusions

We will exclude participants failing (i.e. participants not answering the question or writing anything that does not at least resemble "I pay attention") the following attention check:

> *Imagine you are playing video games with a friend and at some point your friend says: "I don't want to play this game anymore! To make sure that you read the instructions, please write the three following words"I pay attention" in the box below. I really dislike this game, it's the most overrated game ever. Do you agree with your friend?*

# Power analysis

We ran a power simulation to inform our choice of sample size. All assumptions and details on the procedure can be found in the `power_Exp6.Rmd` document. We used previous experiments and estimates of our models to inform our choice of parameter values. 

We ran two different power analyses, one for each outcome variable. We set the power threshold for our experiment to 90%. 

The power simulation for `accuracy` suggested that for 140 participants we would cross the power threshold of 90% for the interaction effect (power = 0.928). The simulation for `competence` suggested that with 300 participants, we would detect an interaction with a power of 87% (power = 0.872). 

Due to budget constraints, we will consider a sample of 300 participants as good enough in terms of power. 

```{r accuracy-image, echo=FALSE, fig.cap="Results of power simulation for competence"}
knitr::include_graphics("power_Exp6_files/figure-html/plot-power-accuracy-1.png")
```

```{r competence-image, echo=FALSE, fig.cap="Results of power simulation for competence"}
knitr::include_graphics("power_Exp6_files/figure-html/plot-power-competence-1.png")
```

## Appendix


```{r stimuli-10, echo=FALSE}
# Create a matrix of image file paths as Markdown-formatted strings
image_paths <- data.frame(condition = c("opposing majority (0)", 
                                        "dissensus (1)", "majority (2)", 
                                        "consensus (10)"),
                          imgage_a = c("![](figures/stimuli/minority_10_a.png){ width=60% }", 
                        "![](figures/stimuli/divergence_10_a.png){ width=60% }",
                        "![](figures/stimuli/majority_10_a.png){ width=60% }", 
                        "![](figures/stimuli/consensus_10_a.png){ width=60% }"),
                        imgage_b = c("![](figures/stimuli/minority_10_b.png){ width=60% }", 
                        "![](figures/stimuli/divergence_10_b.png){ width=60% }",
                        "![](figures/stimuli/majority_10_b.png){ width=60% }", 
                        "![](figures/stimuli/consensus_10_b.png){ width=60% }"))

# Use kable() to create the table and print it as Markdown
kableExtra::kable(image_paths, format = "markdown",
                  col.names = c("Level", "Version a)", "Version b)"), 
                  align = "c",
                  caption = "Stimuli for 10 options condition by levels of convergence")

```

```{r stimuli-10-alt, echo=FALSE}
# Create a matrix of image file paths as Markdown-formatted strings
image_paths <- data.frame(condition = c("opposing majority (0)", 
                                        "dissensus (1)", "majority (2)", 
                                        "consensus (10)"),
                          imgage_a = c("![](figures/stimuli/minority_10_a_alt.png){ width=60% }", 
                        "![](figures/stimuli/divergence_10_a_alt.png){ width=60% }",
                        "![](figures/stimuli/majority_10_a_alt.png){ width=60% }", 
                        "![](figures/stimuli/consensus_10_a_alt.png){ width=60% }"),
                        imgage_b = c("![](figures/stimuli/minority_10_b_alt.png){ width=60% }", 
                        "![](figures/stimuli/divergence_10_b_alt.png){ width=60% }",
                        "![](figures/stimuli/majority_10_b_alt.png){ width=60% }", 
                        "![](figures/stimuli/consensus_10_b_alt.png){ width=60% }"))

# Use kable() to create the table and print it as Markdown
kableExtra::kable(image_paths, format = "markdown",
                  col.names = c("Level", "Version a)", "Version b)"), 
                  align = "c",
                  caption = "Alternative stimuli for 10 options condition by levels of convergence")

```


